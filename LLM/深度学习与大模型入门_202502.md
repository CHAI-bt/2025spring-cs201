## 深度学习与大语言模型入门 2025.02

### 深度学习基础

- 数字角度看世界：一切皆数字，一切智能皆**函数**
  - 如何数值化：文字 → 词表编码，图片 → 像素 RGB，视频 → 连续的帧图片...
  - 图像检测：输入一张图片，输出其中是否包含某个物体、物体的边缘轮廓...
  - 文字、图像生成：输入一段文字，输出这段文字的续写/回答/文字描述的图片...
- 流程：输入（文/图/视频）→ 编码为数值化表示 → 函数进行一系列运算 → 得到数值输出 → 解码为所需格式
- **机器学习/深度学习（Machine Leaning/Deep Learning）**：使用（深层神经网络）模型来实现这些函数
- 早期的机器学习模型：线性分类器
  - 公式：$y=wx+b$
  - 寻找划分不同类别数据点的最佳超平面（$w, b$ 参数取值）
  - 能解决简单的分类任务

- 深度学习模型：神经网络（Neural Network）
  - 一个人工定义的计算图，每个节点设定 $w, b$ 参数，节点输入与这些参数做乘法，加法和非线性运算得到输出（公式：$Activation(wx+ b)$），按图结构向前传递直至得到最终输出。
  - 与生物学上大脑的神经元连接结构相似
  - 节点更多、层数更深（参数量更大）的神经网络能够拟合更复杂的函数（表达能力更强）
  - 数学上可证明神经网络可以拟合任何函数
    - https://www.zhihu.com/question/268384579
- 拟合的方法：训练（Training）
  - 用大量要拟合的函数产生的输入输出数据，使用优化算法，逐步调整模型的参数，直至对任意输入能得到足够接近真实输出的结果
  - 损失函数：用于计算模型输出与真实输出的差距
  - 优化算法：随机梯度下降法（SGD），Adams等
  - 训练不足/过度时会出现模型欠拟合/过拟合
  - 评测：在一组没有用来训练的数据上，测试模型输出的准确性

### 从 Transformer 到生成式大模型

- 自然语言处理（Natural Language Processing, NLP）：计算机科学中重要的研究领域
  - 多种任务：情感分类，机器翻译，文本摘要，...
  - 两大类：判别式 和 生成式
- 2017 年，Transformer 模型结构被提出
  - Encoder-Decoder 结构，可独立/组合使用
  - Encoder：token 序列（文本）进，向量出，适合判别式任务，代表工作：BERT（2018）
  - Decoder：token 序列（文本）进，每次出一个 token，和之前序列拼接算下一个token，循环直至生成了特殊的 <eos> token 结束，适合生成式任务，代表工作：GPT（2018）

- 自然语言可以表示为一种**概率分布**：
  - $P(<bos>北京<eos>)=P(北│<bos>)∗P(京│<bos>北)∗P(<eos>|<bos>北京)$​
  - 词元（Token）：自然语言按 词/子词/字 粒度切分后的单元
  - 一切文本生成任务（无论是创作诗歌、解决数学题、写代码...）都是 **Next Token Prediction** 任务
  - Transformer Decoder 就是为了解决这种任务的模型
- 语言模型的训练
  - 2018 之前：针对每个任务，构造有监督（Supervised）数据集，从零专门训练一个模型
    - 生成模型希望有多任务能力
  - 2018 年，BERT 和 GPT 论文提出 Pre-Training + Fine-Tuning 模式：
  - **预训练**：先在大规模的无监督语料上训练模型的通用语言建模能力
    - 方法：从互联网爬取海量文本，每个句子挖空填词 or 给前半句预测后半句
  - **微调**：然后在特定任务的监督数据集上继续训练模型，提升模型在任务上的性能
  - 在多个任务上的分数大幅超过 SOTA（state-of-the-art）结果

- “大”模型的体现
  - 参数量：BERT (2018): 334M（3.34亿），GPT-3 (2020): 175B（1750亿）, DeepSeek-V3 (2025): 671B（6710亿）
  - 预训练数据量：BERT (2018)：3.7B tokens，GPT-3 (2020)：500B tokens，Llama (2023)：1.4T tokens，DeepSeek-V3 (2025): 14.8T tokens
  - 从 2018 至今，模型的参数规模和训练数据量增长了数千倍
- 为什么需要大模型
  - 随着参数量和训练数据量的增大，模型在语言能力、知识记忆等方面的表现没有出现停滞或倒退，而是不断有提升
  - 涌现能力（Emergent Abilities）：在模型尺寸足够大/训练数据量足够多时，某些方面的能力有突然提升

- 计算硬件：GPU
  - 最初为图形计算设计，拥有强大矩阵算力，与神经的网络计算互通
  - 英伟达（NVIDIA）：行业龙头，最强硬件性能 + 沉淀多年的 CUDA 编程生态
    - RTX 系列：消费级GPU
    - A100/H100 系列：专业GPU
  - Intel Gaudi，华为昇腾等产品正在追赶

- 大模型的“量化”
  - 大部分模型的参数是 float16/bfloat16 格式，每个参数 2 字节（16 bit）
    - DeepSeek-V3/R1 671B 满血版除外，纯 fp8 训练

  - 模型推理需要的显存约为 2.5x~3x 参数量
    - 例：1.5B 模型 ≈ 3.7G - 4.5G，7B 模型 ≈ 17.5 - 21G

  - 模型量化（Quantization）：将参数近似存储为更低字节
    - 小幅牺牲效果，大幅节省显存
    - INT4 = 4 bit 量化，INT8 = 8 bit 量化
    - 量化后模型需要的显存相应除字节压缩倍数

- 当前的主流大模型（2025.02）
  - 商业化模型
    - OpenAI：ChatGPT，GPT-4o，GPT-o1/o3
    - Google：Gemini 2.0 Pro
    - Anthropic：Claude 3.5 Sonnet
    - 百度：文心一言4.0；
    - 月之暗面：Kimi v1, k1.5
  - 开源模型
    - Meta AI (Facebook)：LLaMA-3, 3.1, 3.2, 3.3
    - 阿里：通义千问 Qwen-2.5, 2.5 Coder
    - 深度求索：DeepSeek V3/R1, Distill-Qwen2.5, Distill-Llama3
  - 竞争点：评测指标（数学，代码等能力），长上下文，多模态，长思考，价格

### 大模型本地部署、API 调用

- 本地部署大模型
  - LM Studio：图形化界面，一键下载运行模型
  - Ollama (+ Open WebUI)
    - 注意 tag，ollama 默认下载的是量化模型
  - HuggingFace：最流行的深度学习开源社区，AI 圈的 Github

- 通过 API 调用大模型
  - 为什么需要 API：批量调用，基于大模型调用构建上层应用等
  - OpenAI API 规范：在 ChatGPT 爆火后迅速成为大模型调用的事实标准，所有云平台/本地部署工具都会提供 OpenAI API 格式的调用服务
  - 当前主流文本生成模型的调用参考 Chat Completion 接口的输入输出即可
  - 两种具体调用方法：
    - 使用 OpenAI 库，client.chat.completions 对象
    - 本质是 HTTP 接口，通过 requests 库或 curl 自行组装 json 直接发请求也可以
  - 文档：https://platform.openai.com/docs/api-reference/chat/create